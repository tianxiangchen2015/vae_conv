{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of 400 epochs are set to run\n",
      "option selected 1\n",
      "parameter initialization finished\n",
      "directories creation compledted or if already exist - then checked\n",
      "reding dataset starts\n",
      "option chosen 1\n",
      "frames read: 0\n",
      "initial matrix array dimension: (280000, 28)\n",
      "final matrix array dimension: (10000, 28, 28)\n",
      "dat dimension: (1110000, 3)\n",
      "printing shape of dataset:\n",
      "train set: (8000, 784) (8000, 3)\n",
      "test set: (1000, 784) (1000, 3)\n",
      "pred set: (1000, 784) (1000, 3)\n",
      "data read and shape is now written\n",
      "now building model and compiling\n",
      "reshaping image dimension\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/odb/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:150: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/Users/odb/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:151: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/Users/odb/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:154: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/Users/odb/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:155: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/Users/odb/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:158: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/Users/odb/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:159: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model selection finished\n",
      "optimizers selection finished\n",
      "model compilation finished\n",
      "model summary:\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_3 (InputLayer)             (50, 1, 1, 784)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_21 (Convolution2D) (50, 1, 1, 784)       3           input_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_22 (Convolution2D) (50, 64, 1, 392)      192         convolution2d_21[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_23 (Convolution2D) (50, 64, 1, 392)      36928       convolution2d_22[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_24 (Convolution2D) (50, 64, 1, 392)      36928       convolution2d_23[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)              (50, 25088)           0           convolution2d_24[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)              (50, 25088)           0           flatten_5[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_21 (Dense)                 (50, 128)             3211392     dropout_5[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_22 (Dense)                 (50, 3)               387         dense_21[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_23 (Dense)                 (50, 3)               387         dense_21[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)                (50, 3)               0           dense_22[0][0]                   \n",
      "                                                                   dense_23[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_24 (Dense)                 (50, 128)             512         lambda_5[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_25 (Dense)                 (50, 25088)           3236352     dense_24[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "reshape_5 (Reshape)              (50, 64, 1, 392)      0           dense_25[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "deconvolution2d_13 (Deconvolution(50, 64, 1, 392)      36928       reshape_5[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "deconvolution2d_14 (Deconvolution(50, 64, 1, 392)      36928       deconvolution2d_13[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "deconvolution2d_15 (Deconvolution(50, 64, 1, 784)      8256        deconvolution2d_14[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_25 (Convolution2D) (50, 1, 1, 783)       129         deconvolution2d_15[0][0]         \n",
      "====================================================================================================\n",
      "Total params: 6605322\n",
      "____________________________________________________________________________________________________\n",
      "model building and compilation finished now\n",
      "skipping - no previous saved file to load\n",
      "Train on 8000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "8000/8000 [==============================] - 84s - loss: 183.2646 - val_loss: 150.8547\n",
      "completed 1 epochs\n",
      "saved history files\n",
      "Train on 8000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "8000/8000 [==============================] - 84s - loss: 118.6226 - val_loss: 127.7872\n",
      "completed 2 epochs\n",
      "saved history files\n",
      "Train on 8000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "8000/8000 [==============================] - 79s - loss: 81.5811 - val_loss: 128.8755\n",
      "completed 3 epochs\n",
      "saved history files\n",
      "Train on 8000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "8000/8000 [==============================] - 91s - loss: 71.2854 - val_loss: 126.3411\n",
      "completed 4 epochs\n",
      "saved history files\n",
      "Train on 8000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "8000/8000 [==============================] - 114s - loss: 65.5737 - val_loss: 121.8749\n",
      "completed 5 epochs\n",
      "saved history files\n",
      "Train on 8000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "8000/8000 [==============================] - 125s - loss: 61.6733 - val_loss: 123.9917\n",
      "completed 6 epochs\n",
      "saved history files\n",
      "Train on 8000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "8000/8000 [==============================] - 127s - loss: 59.0560 - val_loss: 123.9057\n",
      "completed 7 epochs\n",
      "saved history files\n",
      "Train on 8000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "8000/8000 [==============================] - 134s - loss: 56.7798 - val_loss: 123.8391\n",
      "completed 8 epochs\n",
      "saved history files\n",
      "Train on 8000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "8000/8000 [==============================] - 157s - loss: 55.1456 - val_loss: 121.5913\n",
      "completed 9 epochs\n",
      "saved history files\n",
      "Train on 8000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "8000/8000 [==============================] - 166s - loss: 53.6079 - val_loss: 122.7948\n",
      "completed 10 epochs\n",
      "saved history files\n",
      "Train on 8000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "8000/8000 [==============================] - 163s - loss: 52.1354 - val_loss: 123.4523\n",
      "completed 11 epochs\n",
      "saved history files\n",
      "Train on 8000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "8000/8000 [==============================] - 165s - loss: 50.9860 - val_loss: 131.2768\n",
      "completed 12 epochs\n",
      "saved history files\n",
      "Train on 8000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "8000/8000 [==============================] - 165s - loss: 49.9881 - val_loss: 129.9666\n",
      "completed 13 epochs\n",
      "saved history files\n",
      "Train on 8000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "8000/8000 [==============================] - 163s - loss: 48.9696 - val_loss: 135.7664\n",
      "completed 14 epochs\n",
      "saved history files\n",
      "Train on 8000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "8000/8000 [==============================] - 157s - loss: 48.1645 - val_loss: 136.5078\n",
      "completed 15 epochs\n",
      "saved history files\n",
      "Train on 8000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "8000/8000 [==============================] - 129s - loss: 47.4171 - val_loss: 142.6362\n",
      "completed 16 epochs\n",
      "saved history files\n",
      "Train on 8000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "8000/8000 [==============================] - 128s - loss: 46.6971 - val_loss: 145.5039\n",
      "completed 17 epochs\n",
      "saved history files\n",
      "Train on 8000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "8000/8000 [==============================] - 128s - loss: 45.9776 - val_loss: 150.6586\n",
      "completed 18 epochs\n",
      "saved history files\n",
      "Train on 8000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "8000/8000 [==============================] - 128s - loss: 45.3431 - val_loss: 148.0527\n",
      "completed 19 epochs\n",
      "saved history files\n",
      "Train on 8000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "8000/8000 [==============================] - 131s - loss: 44.8112 - val_loss: 154.3401\n",
      "completed 20 epochs\n",
      "saved history files\n",
      "Train on 8000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "8000/8000 [==============================] - 157s - loss: 44.2172 - val_loss: 156.5799\n",
      "completed 21 epochs\n",
      "saved history files\n",
      "Train on 8000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "8000/8000 [==============================] - 164s - loss: 43.7439 - val_loss: 152.7221\n",
      "completed 22 epochs\n",
      "saved history files\n",
      "Train on 8000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "8000/8000 [==============================] - 163s - loss: 43.1948 - val_loss: 161.8476\n",
      "completed 23 epochs\n",
      "saved history files\n",
      "Train on 8000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "8000/8000 [==============================] - 166s - loss: 42.7359 - val_loss: 166.7623\n",
      "completed 24 epochs\n",
      "saved history files\n",
      "Train on 8000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "8000/8000 [==============================] - 165s - loss: 42.2879 - val_loss: 155.9764\n",
      "completed 25 epochs\n",
      "saved history files\n",
      "Train on 8000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "8000/8000 [==============================] - 165s - loss: 41.8909 - val_loss: 167.1202\n",
      "completed 26 epochs\n",
      "saved history files\n",
      "Train on 8000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "8000/8000 [==============================] - 159s - loss: 41.5131 - val_loss: 166.4869\n",
      "completed 27 epochs\n",
      "saved history files\n",
      "Train on 8000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "8000/8000 [==============================] - 130s - loss: 41.0605 - val_loss: 178.5735\n",
      "completed 28 epochs\n",
      "saved history files\n",
      "Train on 8000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "8000/8000 [==============================] - 164s - loss: 40.6850 - val_loss: 178.1982\n",
      "completed 29 epochs\n",
      "saved history files\n",
      "Train on 8000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "8000/8000 [==============================] - 170s - loss: 40.3878 - val_loss: 178.3151\n",
      "completed 30 epochs\n",
      "saved history files\n",
      "Train on 8000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "8000/8000 [==============================] - 168s - loss: 39.9821 - val_loss: 180.5083\n",
      "completed 31 epochs\n",
      "saved history files\n",
      "Train on 8000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      " 350/8000 [>.............................] - ETA: 153s - loss: 39.1895"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-e3d62d4fdf22>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[0;31m#        verbose=2,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m         callbacks=[history])       \n\u001b[0m\u001b[1;32m    377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m     \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./model/model_%i'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/odb/anaconda/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight)\u001b[0m\n\u001b[1;32m   1104\u001b[0m                               \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1106\u001b[0;31m                               callback_metrics=callback_metrics)\n\u001b[0m\u001b[1;32m   1107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/odb/anaconda/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, nb_epoch, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics)\u001b[0m\n\u001b[1;32m    822\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 824\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    825\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/odb/anaconda/lib/python2.7/site-packages/keras/backend/theano_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    715\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 717\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/odb/anaconda/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'position_of_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# selecting parameters\n",
    "# number of convolutional filters to use\n",
    "nb_filters = 64 \n",
    "\n",
    "# filter size    \n",
    "filter_rows = 1\n",
    "filter_cols = 2 \n",
    "\n",
    "# convolution kernel size\n",
    "nb_conv = 3\n",
    "\n",
    "# subsampling size \n",
    "subsample_rows = 1 \n",
    "subsample_cols_a = 2    \n",
    "subsample_cols_b = 1\n",
    "\n",
    "# batch size \n",
    "batch_size = 50\n",
    "\n",
    "# dimension    \n",
    "latent_dim = 3\n",
    "intermediate_dim = 128\n",
    "\n",
    "# epsilon values \n",
    "mean=0.\n",
    "epsilon_std = 1.0      \n",
    "\n",
    "# dropout \n",
    "drop = 0\n",
    "\n",
    "# epoch, iteration sizes  \n",
    "nb_epoch = 1  \n",
    "\n",
    "# saving models: start & end\n",
    "nb_start = 0\n",
    "nb_end = 400       \n",
    "total_epochs = (nb_end-nb_start)*nb_epoch  \n",
    "print \"Total of %i epochs are set to run\" % total_epochs  \n",
    "\n",
    "# selecting path for array and data file (if saved in text or in pickle or compressed file)   \n",
    "opt = 1  \n",
    "# depending on 'opt' value choose path_data, path_data_array or path_data_dat    \n",
    "# if 'opt=1' then choose path_data_array and path_data_dat, n_traj, f_traj, row_dim and col_dim     \n",
    "# else for 'opt=2' or 'opt=3' choose path_data\n",
    "if opt == 1:\n",
    "    # number of trajectory files\n",
    "    n_traj = 1#111\n",
    "    # number of frames per trajectories\n",
    "    f_traj = 10000\n",
    "    # row and column dimension for each frame in the array file \n",
    "    row_dim_array = 28    \n",
    "    col_dim_array = 28   \n",
    "    # fraction of train, test and pred data separation \n",
    "    sep_train = 0.8     \n",
    "    sep_test = 0.9    \n",
    "    sep_pred = 1        \n",
    "    # path of array and dat file \n",
    "    path_data_array = \"/Users/odb/Documents/deeplearning_run/1FME-0/analysis/metadata/1FME-0_cont-mat.array\"\n",
    "    path_data_dat = '/Users/odb/Documents/deeplearning_run/1FME-0/analysis/metadata/1FME-0_cont-mat.dat'    \n",
    "    print \"option selected\", opt\n",
    "\n",
    "if opt != 1:    \n",
    "    # fraction of train, test and pred data separation \n",
    "    sep_train = 0.8     \n",
    "    sep_test = 0.9    \n",
    "    sep_pred = 1        \n",
    "    # path of data file \n",
    "    path_data = './X_110000.pkl.gz'  \n",
    "    print \"option selected\", opt\n",
    "\n",
    "#\n",
    "print \"parameter initialization finished\"\n",
    "#\n",
    "###########################################################################################################\n",
    "#\n",
    "#!/usr/bin/python\n",
    "\n",
    "import os, sys\n",
    "\n",
    "path_1 = \"./fig\"\n",
    "path_2 = \"./imgs\"\n",
    "path_3 = \"./hist\"\n",
    "path_4 = \"./model\"\n",
    "if not os.path.exists(path_1):\n",
    "    os.mkdir(path_1, 0755);\n",
    "if not os.path.exists(path_2):\n",
    "    os.mkdir(path_2, 0755);\n",
    "if not os.path.exists(path_3):\n",
    "    os.mkdir(path_3, 0755);\n",
    "if not os.path.exists(path_4):\n",
    "    os.mkdir(path_4, 0755);   \n",
    "print \"directories creation compledted or if already exist - then checked\"    \n",
    "\n",
    "    \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gzip\n",
    "from six.moves import cPickle\n",
    "import sys    \n",
    "\n",
    "   \n",
    "# reading dataset    \n",
    "print \"reding dataset starts\"    \n",
    "print \"option chosen %i\" % opt    \n",
    "\n",
    "if opt == 1:  \n",
    "    # option 1 \n",
    "    # open separate array and data files stored in text file      \n",
    "    # total number of frames        \n",
    "    - \n",
    "\n",
    "if opt == 2:\n",
    "    # option 2    \n",
    "    # open compressed pickled data already separated in train, test and prediction data          \n",
    "    with gzip.open(path_data, 'rb') as f3:    \n",
    "        (x_train_raw, y_train_raw), (x_test_raw,y_test_raw), (x_pred_raw,y_pred_raw) = cPickle.load(f3)     \n",
    "\n",
    "if opt == 3:\n",
    "    # option 3 \n",
    "    # open compressed pickled data not separated in train, test and prediction data     \n",
    "    with gzip.open(path_data, 'rb') as f3:    \n",
    "        (x_raw, y_raw) = cPickle.load(f3)  \n",
    "    \n",
    "    # determining size for training, testing & prediction data    \n",
    "    sep_1 = x_raw.shape[0]*sep_train \n",
    "    sep_2 = x_raw.shape[0]*sep_test    \n",
    "    sep_3 = x_raw.shape[0]*sep_pred   \n",
    "    \n",
    "    # assigning training set \n",
    "    x_train_raw = x_raw[:sep_1]\n",
    "    y_train_raw = y_raw[:sep_1]  \n",
    "    \n",
    "    # assigning testing set \n",
    "    x_test_raw = x_raw[sep_1:sep_2]\n",
    "    y_test_raw = y_raw[sep_1:sep_2] \n",
    "    \n",
    "    # assigning prediction set \n",
    "    x_pred_raw = x_raw[sep_2:sep_3]\n",
    "    y_pred_raw = y_raw[sep_2:sep_3]  \n",
    "\n",
    "\n",
    "print 'printing shape of dataset:'\n",
    "print 'train set:', np.shape(x_train_raw), np.shape(y_train_raw) \n",
    "print 'test set:', np.shape(x_test_raw), np.shape(y_test_raw) \n",
    "print 'pred set:', np.shape(x_pred_raw), np.shape(y_pred_raw)\n",
    "print \"data read and shape is now written\"\n",
    "\n",
    "\n",
    "# model building and compiling\n",
    "print \"now building model and compiling\"\n",
    "import keras\n",
    "from scipy.stats import norm\n",
    "from keras.layers import Input, Dense, Lambda, Flatten, Reshape, Dropout\n",
    "from keras.layers import Convolution2D, Deconvolution2D\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras import objectives\n",
    "from keras.datasets import mnist   \n",
    "from keras.optimizers import SGD, Adam, RMSprop, Adadelta\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "\n",
    "\n",
    "# normalizing input image matrix \n",
    "a = np.amax(x_train_raw)\n",
    "b = np.amax(x_test_raw)\n",
    "c = np.amax(x_pred_raw)\n",
    "x_train = x_train_raw.astype('float32') / a\n",
    "x_test = x_test_raw.astype('float32') / b\n",
    "x_pred = x_pred_raw.astype('float32') / c\n",
    "\n",
    "# input image dimensions   \n",
    "print \"reshaping image dimension\"\n",
    "img_rows = x_train_raw.shape[0]/len(x_train_raw)    \n",
    "img_cols = x_train_raw.shape[1]\n",
    "img_rows, img_cols, img_chns = img_rows, img_cols, 1\n",
    "\n",
    "if K.image_dim_ordering() == 'th':\n",
    "    original_img_size = (img_chns, img_rows, img_cols)\n",
    "else:\n",
    "    original_img_size = (img_rows, img_cols, img_chns)\n",
    "\n",
    "x_train = x_train.reshape((x_train.shape[0],) + original_img_size)\n",
    "x_test = x_test.reshape((x_test.shape[0],) + original_img_size)\n",
    "x_pred = x_pred.reshape((x_pred.shape[0],) + original_img_size)  \n",
    "\n",
    "x = Input(batch_shape=(batch_size,) + original_img_size)\n",
    "\n",
    "\n",
    "# defining convolutional variational autoencoders        \n",
    "'''building variational autoencoder with Keras and deconvolution layers.\n",
    "Reference: Variational Autoencoder: \"Auto-Encoding Variational Bayes\" (https://arxiv.org/abs/1312.6114)\n",
    "'''\n",
    "def vae_conv():    \n",
    "    conv_1 = Convolution2D(img_chns, filter_rows, filter_cols, border_mode='same', activation='relu')(x)\n",
    "    conv_2 = Convolution2D(nb_filters, filter_rows, filter_cols,\n",
    "                       border_mode='same', activation='relu',\n",
    "                       subsample=(subsample_rows, subsample_cols_a))(conv_1)\n",
    "    conv_3 = Convolution2D(nb_filters, nb_conv, nb_conv,\n",
    "                       border_mode='same', activation='relu',\n",
    "                       subsample=(subsample_rows, subsample_cols_b))(conv_2)\n",
    "    conv_4 = Convolution2D(nb_filters, nb_conv, nb_conv,\n",
    "                       border_mode='same', activation='relu',\n",
    "                       subsample=(subsample_rows, subsample_cols_b))(conv_3)\n",
    "    flat = Flatten()(conv_4)\n",
    "    flat_d = Dropout(drop)(flat)\n",
    "    hidden = Dense(intermediate_dim, activation='relu')(flat_d)\n",
    "\n",
    "    z_mean = Dense(latent_dim)(hidden)\n",
    "    z_log_var = Dense(latent_dim)(hidden)        \n",
    "\n",
    "    # sampling on latent space \n",
    "    def sampling(args):\n",
    "        z_mean, z_log_var = args\n",
    "        epsilon = K.random_normal(shape=(batch_size, latent_dim),\n",
    "                              mean=mean, std=epsilon_std)\n",
    "        return z_mean + K.exp(z_log_var) * epsilon\n",
    "\n",
    "    z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "\n",
    "    # we instantiate these layers separately so as to reuse them later\n",
    "    decoder_hid = Dense(intermediate_dim, activation='relu')\n",
    "    decoder_upsample = Dense(nb_filters * img_rows * (img_cols/2), activation='relu')\n",
    "\n",
    "    if K.image_dim_ordering() == 'th':\n",
    "        output_shape = (batch_size, nb_filters, img_rows, (img_cols/2))\n",
    "    else:\n",
    "        output_shape = (batch_size, img_rows, (img_cols/2), nb_filters)\n",
    "\n",
    "    decoder_reshape = Reshape(output_shape[1:])\n",
    "    decoder_deconv_1 = Deconvolution2D(nb_filters, nb_conv, nb_conv,\n",
    "                                   output_shape,\n",
    "                                   border_mode='same',\n",
    "                                   subsample=(subsample_rows, subsample_cols_b),\n",
    "                                   activation='relu')\n",
    "    decoder_deconv_2 = Deconvolution2D(nb_filters, nb_conv, nb_conv,\n",
    "                                   output_shape,\n",
    "                                   border_mode='same',\n",
    "                                   subsample=(subsample_rows, subsample_cols_b),\n",
    "                                   activation='relu')\n",
    "    if K.image_dim_ordering() == 'th':\n",
    "        output_shape = (batch_size, nb_filters, img_rows, (img_cols+1))\n",
    "    else:\n",
    "        output_shape = (batch_size, img_rows, (img_cols+1), nb_filters)\n",
    "    decoder_deconv_3_upsamp = Deconvolution2D(nb_filters, filter_rows, filter_cols,\n",
    "                                          output_shape,\n",
    "                                          border_mode='valid',\n",
    "                                          subsample=(subsample_rows, subsample_cols_a),\n",
    "                                          activation='relu')\n",
    "    decoder_mean_squash = Convolution2D(img_chns, filter_rows, filter_cols,\n",
    "                                    border_mode='valid',\n",
    "                                    activation='sigmoid')\n",
    "\n",
    "    hid_decoded = decoder_hid(z)\n",
    "    up_decoded = decoder_upsample(hid_decoded)\n",
    "    reshape_decoded = decoder_reshape(up_decoded)\n",
    "    deconv_1_decoded = decoder_deconv_1(reshape_decoded)\n",
    "    deconv_2_decoded = decoder_deconv_2(deconv_1_decoded)\n",
    "    x_decoded_relu = decoder_deconv_3_upsamp(deconv_2_decoded)\n",
    "    x_decoded_mean_squash = decoder_mean_squash(x_decoded_relu)  \n",
    "    \n",
    "    return z_mean, z_log_var, x_decoded_mean_squash\n",
    "\n",
    "def vae_loss(x, x_decoded_mean):\n",
    "    vae_conv_return = vae_conv()\n",
    "    z_mean, z_log_var, x_decoded_mean_squash = vae_conv_return\n",
    "    # NOTE: binary_crossentropy expects a batch_size by dim\n",
    "    # for x and x_decoded_mean, so we MUST flatten these!\n",
    "    x = K.flatten(x)\n",
    "    x_decoded_mean = K.flatten(x_decoded_mean)\n",
    "    xent_loss = img_rows * img_cols * objectives.binary_crossentropy(x, x_decoded_mean)\n",
    "    kl_loss = - 0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "    \n",
    "    return xent_loss + kl_loss\n",
    "\n",
    "# model to run \n",
    "vae_conv_return = vae_conv()\n",
    "z_mean, z_log_var, x_decoded_mean_squash = vae_conv_return\n",
    "vae = Model(x, x_decoded_mean_squash)   \n",
    "print \"model selection finished\"\n",
    "\n",
    "\n",
    "# Optimizers selection  \n",
    "rms = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "#adadelta = Adadelta(lr=1.0, rho=0.95, epsilon=1e-08, decay=0.0)  \n",
    "print \"optimizers selection finished\"\n",
    "\n",
    "# compiling model \n",
    "vae.compile(optimizer=rms, loss=vae_loss)\n",
    "print \"model compilation finished\"\n",
    "\n",
    "# model summary \n",
    "print \"model summary:\"  \n",
    "vae.summary()     \n",
    "print \"model building and compilation finished now\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# saving history \n",
    "class LossHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "\n",
    "history = LossHistory()\n",
    "\n",
    "# 1) fitting model with training dataset, saving model & loading for next fitting    \n",
    "# 2) saving loss values \n",
    "for i in range (nb_start, nb_end):    \n",
    "    if i == 0:        \n",
    "        print('skipping - no previous saved file to load')\n",
    "        \n",
    "    else:        \n",
    "        vae.load_weights('./model/model_%i' % i)       \n",
    "                \n",
    "    vae.fit(x_train, x_train,\n",
    "        shuffle=True,\n",
    "        nb_epoch=nb_epoch,\n",
    "        batch_size=batch_size,\n",
    "#        verbose=2,\n",
    "        validation_data=(x_test, x_test),\n",
    "        callbacks=[history])       \n",
    "    \n",
    "    vae.save_weights('./model/model_%i' % (i+1))\n",
    "    print('completed %i epochs' % ((i+1)*nb_epoch)) \n",
    "    \n",
    "    np.savetxt('./hist/history.losses_%i' %(i+1), history.losses, delimiter=',')\n",
    "    np.savetxt('./hist/history.val_losses_%i' %(i+1), history.val_losses, delimiter=',')\n",
    "    print \"saved history files\"    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# finishing the job \n",
    "print \"the job is finsihed. You can find the model and the history files in the respective folders.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
